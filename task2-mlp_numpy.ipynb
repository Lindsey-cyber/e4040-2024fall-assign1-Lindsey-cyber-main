{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qe6yTh55trpQ"
   },
   "source": [
    "# Assignment 1, Task 2: Multi-Layer Perceptron (MLP)\n",
    "\n",
    "You will get to know how to build basic fully connected (FC) neural network. In this task, all the functions will be created from scratch using NumPy for your understanding. You will be introduced to built-in layers from TensorFlow in Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Vs2WYIFtrpS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gYnTjputrpV"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "I31uJ6KltrpW",
    "outputId": "1a677958-43ca-422c-ec66-38e8cdff4283",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 784) (1000, 784) (10000, 784) (100, 784)\n",
      "Train data shape:  (49000, 784)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 784)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 784)\n",
      "Test labels shape:  (10000,)\n",
      "Development data shape: (100, 784)\n",
      "Development data shape (100,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw Fashion-MNIST data.\n",
    "train, test = fashion_mnist.load_data()\n",
    "\n",
    "X_train_raw, y_train = train\n",
    "X_test_raw, y_test = test\n",
    "\n",
    "X_train = X_train_raw.reshape((X_train_raw.shape[0], X_train_raw.shape[1]**2))\n",
    "X_test = X_test_raw.reshape((X_test_raw.shape[0], X_test_raw.shape[1]**2))\n",
    "\n",
    "# Data organizations:\n",
    "# Train data: 49000 samples from original train set: 1~49,000\n",
    "# Validation data: 1000 samples from original train set: 49,000~50,000\n",
    "# Test data: 10000 samples from original test set: 1~10,000\n",
    "# Development data (for gradient check): 100 from the train set: 1~49,000\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_dev = 100\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "X_test = X_test.astype(np.float32) - mean_image\n",
    "X_dev = X_dev.astype(np.float32) - mean_image\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development data shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLhMsud9trpa"
   },
   "source": [
    "## Part 1: Basic Layers (15%)\n",
    "\n",
    "A neural network layer needs two different passes in order to function properly:\n",
    "- **Forward Pass**: computes the outputs given inputs\n",
    "- **Backward Pass**: computes the gradients w.r.t. inputs and parameters from the outputs\n",
    "\n",
    "In this part, we will implement both passes for several typical MLP components, which includes the **affine layer** (to generate linear embeddings of the inputs) and the **activation layer** (to provide non-linearity). Then we will implement the **Cross-Entropy Layer**, which is a one of the most popular classification loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider some input $X \\in R^{N \\times D}$ (from the previous layer), the forward pass of an affine layer with weight $W \\in R^{D \\times M}$ and bias $b \\in R^M$ is defined as\n",
    "\n",
    "$$Y = X W + \\mathbb{1} b^T \\in R^{N \\times M}$$\n",
    "\n",
    "During model training, we seek to update the parameters in order to decrease the loss. This requires the gradient of the loss $L$ w.r.t. the layer parameters $W$ and $b$. Assume that the gradient of $L$ w.r.t. the layer output $Y$ is given as\n",
    "\n",
    "$$\\nabla_Y L = G \\in R^{N \\times M}$$\n",
    "\n",
    "The gradients of $L$ w.r.t. $W$ and $b$ can be computed as\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\nabla_W L = X^T G \\in R^{D \\times M} \\\\\n",
    "\\nabla_b L = \\mathbb{1}^T G \\in R^M\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "For other layers preceeding the current one, we would also want to update their parameters with the help of the gradients w.r.t. their output, which is our input $X$. It writes similarly that\n",
    "\n",
    "$$\\nabla_X L = G W^T \\in R^{N \\times D}$$\n",
    "\n",
    "For those who are interested in how the equations are formulated, refer to http://cs231n.stanford.edu/handouts/linear-backprop.pdf.\n",
    "\n",
    "An important observation is that the calculation of the gradients of the current layer depends on the **upstream gradients** (i.e. $\\nabla_Y L$) from the layer afterwards. One needs to calculate the gradient of the loss value w.r.t. each parameter from the final output layer all the way back to the very beginning. This process is called **back-propagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete the functions `affine_forward`, `affine_backward` in `./utils/layer_funcs.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below. The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "2AfAMrOZtrpb",
    "outputId": "af228bb7-f1eb-41bf-e9f5-c3a6de6cdbd5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is out correct? True\n",
      "Is dx correct? True\n",
      "Is dw correct? True\n",
      "Is db correct? True\n"
     ]
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.layer_funcs import affine_forward\n",
    "from utils.layer_funcs import affine_backward\n",
    "\n",
    "# Generate data for checking\n",
    "x = X_dev\n",
    "w = np.random.rand(x.shape[1],100)\n",
    "b = np.random.rand(100)\n",
    "dout = np.ones((x.shape[0],100))\n",
    "\n",
    "## Affine function: H = W*X + b\n",
    "out = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, x, w, b)\n",
    "\n",
    "## Check your implementation using the tf.gradients_function()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "w_tf = tf.Variable(w, name='w')\n",
    "b_tf = tf.Variable(b, name='b')\n",
    "\n",
    "def affine_layer(x, w, b):\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(w_tf)\n",
    "    out_tf = affine_layer(x_tf, w_tf, b_tf)\n",
    "    dx_tf, dw_tf, db_tf = tape.gradient(out_tf, (x_tf, w_tf, b_tf))\n",
    "\n",
    "out_check = out_tf.numpy()\n",
    "dx_check, dw_check, db_check = dx_tf.numpy(), dw_tf.numpy(), db_tf.numpy()\n",
    "\n",
    "## Print validation results\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))\n",
    "print(\"Is dw correct? {}\".format(np.allclose(dw, dw_check)))\n",
    "print(\"Is db correct? {}\".format(np.allclose(db, db_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-linearity is crucial for neural networks in their capabilities to learn and represent complex data. Mathematically, cascading linear maps are essentially equivalent to a single linear map, which is why we seek to inject non-linear functions between them. One of the simplest is ReLU (Rectified Linear Unit).\n",
    "\n",
    "Consider the following function that admits a scaler input $x$. The forward pass of `ReLU` is defined as\n",
    "\n",
    "$$y = \\max (x, 0)$$\n",
    "\n",
    "This is an element-wise mapping that can be propagated to arbitrary dimensions. The function mimics the activation mechnism of biological neurons which suppress negative inputs and pass through positive ones.\n",
    "\n",
    "The backward pass is fairly straight forward. Assume that the gradient of loss $L$ w.r.t. the layer output $y$ is given as\n",
    "\n",
    "$$\\nabla_y L = g \\in R$$\n",
    "\n",
    "Since this layer is non-parametric (no internal parameters), we only care about the gradients of $L$ w.r.t. layer input $x$, which writes\n",
    "\n",
    "$$\n",
    "\\nabla_x L = \\begin{cases}\n",
    "g & \\quad \\text{if } x > 0 \\\\\n",
    "0 & \\quad \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LL0DqZ_ftrpd"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete the functions `relu_forward`, `relu_backward` in `./utils/layer_funcs.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below. The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "nYDT8Idatrpe",
    "outputId": "a1db7202-1da9-45f5-bde1-ee42b623029f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is out correct? True\n",
      "Is dx correct? True\n"
     ]
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.layer_funcs import relu_forward\n",
    "from utils.layer_funcs import relu_backward\n",
    "\n",
    "## Activation layers -- Here we introduce ReLU activation function\n",
    "## since it is the most commonly used in computer vision problems.\n",
    "## However, you can also try to implement \n",
    "## other activation functions like sigmoid, tanh etc.\n",
    "x = X_dev\n",
    "dout = np.ones(x.shape)\n",
    "\n",
    "## ReLU\n",
    "out = relu_forward(x)\n",
    "dx = relu_backward(dout, x)\n",
    "\n",
    "## Check by tf.GradientTape.gradients()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x_tf)\n",
    "    out_tf = tf.nn.relu(x_tf)\n",
    "    grad_gt = tape.gradient(out_tf, x_tf)\n",
    "\n",
    "out_check = out_tf.numpy()\n",
    "dx_check = grad_gt.numpy()\n",
    "## Print validation result\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another commonly used non-linear activation function is `tanh` (hyperbolic tangent), which is defined as:\n",
    "\n",
    "$$\n",
    "y = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "This function maps input values to a range between -1 and 1, helping to center the data. The `tanh` activation is particularly useful when negative values are significant, as it outputs both positive and negative values.\n",
    "\n",
    "For the backward pass, given a gradient $g$ with respect to the output $y$, the gradient with respect to the input $x$ is:\n",
    "\n",
    "$$\n",
    "\\nabla_x L = g \\cdot (1 - y^2)\n",
    "$$\n",
    "\n",
    "This derivative shows how the gradients flow through the `tanh` activation, modulating the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete the functions `tanh_forward`, `tanh_backward` in `./utils/layer_funcs.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below. The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is out correct? True\n",
      "Is dx correct? True\n"
     ]
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.layer_funcs import tanh_forward\n",
    "from utils.layer_funcs import tanh_backward\n",
    "\n",
    "## Activation layers -- Here we introduce the tanh activation function.\n",
    "x = X_dev\n",
    "dout = np.ones(x.shape)\n",
    "\n",
    "## Tanh\n",
    "out = tanh_forward(x)\n",
    "dx = tanh_backward(dout, x)\n",
    "\n",
    "## Check by tf.GradientTape.gradients()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x_tf)\n",
    "    out_tf = tf.nn.tanh(x_tf)\n",
    "    grad_gt = tape.gradient(out_tf, x_tf)\n",
    "\n",
    "out_check = out_tf.numpy()\n",
    "dx_check = grad_gt.numpy()\n",
    "\n",
    "## Print validation result\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Cross-entropy Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions are real-valued function which represent measurements of how far the model predictions are away from the ground truths. Usually in classification tasks, both the predictions and the ground-truths are given as discrete $K$-dimensional distributions (corresponds to $K$ different classes).\n",
    "\n",
    "As you may recall from task 1, a measure of dissimilarity can be given by the cross-entropy. We provided in task 1 a more detailed explaination so here we simply present the formulas.\n",
    "\n",
    "Recall the softmax function\n",
    "\n",
    "$$\\sigma (x) = \\frac{e^{x}}{\\sum_i e^{x_i}}: R^K \\to (0, 1)^K$$\n",
    "\n",
    "and the cross entropy function\n",
    "\n",
    "$$H(p, q) = -p^T \\log q: R^K \\to R$$\n",
    "\n",
    "Assume the predictions $Y \\in R^{N \\times K}$ ($y_i \\in R^K$ is the predicted logit over all classes in $R^K$ for the $i$-th data sample) and the ground truth $G \\in R^{N \\times K}$ ($g_i \\in R^K$ is the corresponding one-hot encoding of the ground truth class). The forward pass of softmax cross-entropy loss is defined as\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_i H(g_i, \\sigma (y_i))$$\n",
    "\n",
    "And the backward pass is\n",
    "\n",
    "$$\\nabla_Y L = \\frac{1}{N} (\\sigma (Y) - G)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LrSXJOdktrph"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete the functions `softmax_loss` in `./utils/layer_funcs.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below. The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "rG2_zSsjtrpi",
    "outputId": "5b8fd3ce-0175-4aa2-c29d-b0925dcf2f42",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is loss correct? True\n",
      "Is dx correct? True\n"
     ]
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.layer_funcs import softmax_loss\n",
    "\n",
    "## Generate some random data for testing\n",
    "x = np.random.rand(100, 10)\n",
    "y = np.argmax(x, axis=1)\n",
    "\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "## Check by tf.GradientTape.gradients()\n",
    "\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "y_tf = tf.Variable(y, name='y')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x_tf)\n",
    "    loss_tf = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x_tf, labels=tf.one_hot(y_tf, 10)))\n",
    "    dx_tf = tape.gradient(loss_tf, x_tf)\n",
    "\n",
    "loss_check = loss_tf.numpy()\n",
    "dx_check = dx_tf.numpy()\n",
    "## Print validation result\n",
    "print(\"Is loss correct? {}\".format(np.allclose(loss, loss_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a single layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to implement an **affine layer** and then a **dense layer** on top of that. \n",
    "\n",
    "* input >> AffineLayer >> output\n",
    "\n",
    "```\n",
    "    class AffineLayer:\n",
    "        __init__:\n",
    "            params - weights and bias\n",
    "            cache - intermeidate results for back propagation\n",
    "            gradients - gradients of the parameters for optimization\n",
    "        feedforward: forward pass\n",
    "        backward: backward pass\n",
    "        update_layer: update layer parameters\n",
    "```\n",
    "\n",
    "* input >> AffineLayer >> ReLU >> output\n",
    "\n",
    "```\n",
    "    class DenseLayer:\n",
    "        __init__:\n",
    "            affine - an affine layer\n",
    "            activation: activation function\n",
    "        feedforward: forward pass\n",
    "        backward: backward pass\n",
    "        update_layer: update layer parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhIgKu9ptrpl"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete function `AffineLayer` in `./utils/layer_utils.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below. The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "Se8aWo7Ktrpm",
    "outputId": "803e04f9-7654-4ad4-8050-90e0f0e6a2c5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is out correct? True\n",
      "Is dx correct? True\n",
      "Is dw correct? True\n",
      "Is db correct? True\n"
     ]
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.layer_utils import AffineLayer\n",
    "\n",
    "## Affine\n",
    "test_affine = AffineLayer(input_dim=X_train.shape[1], output_dim=100)\n",
    "w, b = test_affine.params['W'], test_affine.params['b']\n",
    "\n",
    "## Data for correctness check\n",
    "x = X_dev\n",
    "dout = np.ones((x.shape[0], 100))\n",
    "\n",
    "out = test_affine.feedforward(x)\n",
    "dx = test_affine.backward(dout)\n",
    "dw, db = test_affine.gradients['W'], test_affine.gradients['b']\n",
    "\n",
    "## Check by tf.GradientTape.gradients()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "w_tf = tf.Variable(w, name='w')\n",
    "b_tf = tf.Variable(b, name='b')\n",
    "\n",
    "def affine_layer(x, w, b):\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(w_tf)\n",
    "    out_tf = affine_layer(x_tf, w_tf, b_tf)\n",
    "    dx_tf, dw_tf, db_tf = tape.gradient(out_tf, (x_tf, w_tf, b_tf))\n",
    "\n",
    "out_check = out_tf.numpy()\n",
    "dx_check = dx_tf.numpy()\n",
    "dw_check = dw_tf.numpy()\n",
    "db_check = db_tf.numpy()\n",
    "\n",
    "## Print validation result\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))\n",
    "print(\"Is dw correct? {}\".format(np.allclose(dw, dw_check)))\n",
    "print(\"Is db correct? {}\".format(np.allclose(db, db_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2MhDHIJtrpo"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete function `DenseLayer` in **./utils/layer_utils.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below. The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "G8MgO2Gztrpq",
    "outputId": "5047dedc-a265-437f-882e-24adfd691074",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is out correct? True\n",
      "Is dx correct? True\n",
      "Is dw correct? True\n",
      "Is db correct? True\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "## First, let's make a dense layer\n",
    "from utils.layer_utils import DenseLayer\n",
    "\n",
    "## Affine + ReLU\n",
    "test_dense = DenseLayer(input_dim=X_train.shape[1], output_dim=100)\n",
    "w, b = test_dense.params['W'], test_dense.params['b']\n",
    "\n",
    "## Data for correctness check\n",
    "x = X_dev\n",
    "dout = np.ones((x.shape[0], 100))\n",
    "\n",
    "z = x.dot(w) + b\n",
    "\n",
    "out = test_dense.feedforward(x)\n",
    "dx = test_dense.backward(dout)\n",
    "dw, db = test_dense.gradients['W'], test_dense.gradients['b']\n",
    "\n",
    "## Check by tf.GradientTape.gradients()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "w_tf = tf.Variable(w, name='w')\n",
    "b_tf = tf.Variable(b, name='b')\n",
    "\n",
    "def dense_layer(x, w, b):\n",
    "    return tf.nn.relu(tf.matmul(x, w) + b)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(w_tf)\n",
    "    out_tf = dense_layer(x_tf, w_tf, b_tf)\n",
    "    dx_tf, dw_tf, db_tf = tape.gradient(out_tf, (x_tf, w_tf, b_tf))\n",
    "\n",
    "out_check = out_tf.numpy()\n",
    "dx_check = dx_tf.numpy()\n",
    "dw_check = dw_tf.numpy()\n",
    "db_check = db_tf.numpy()\n",
    "\n",
    "## Print validation result\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))\n",
    "print(\"Is dw correct? {}\".format(np.allclose(dw, dw_check)))\n",
    "print(\"Is db correct? {}\".format(np.allclose(db, db_check)))\n",
    "\n",
    "# Visualization\n",
    "\n",
    "# Plot the output before and after activation\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(z.flatten(), bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Output before Activation (z)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(out.flatten(), bins=50, color='green', alpha=0.7)\n",
    "plt.title('Output after Activation (ReLU)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: What do you observe from the figure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Your answer: **[fill in here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Two Layer Network (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y9cJLCPstrpt"
   },
   "source": [
    "Complete the class `TwoLayerNet` in `./utils/classifiers/twolayernet.py`. Through this experiment, you will create a two-layer neural network and learn about the backpropagation mechanism. The network structure is like \n",
    "\n",
    "* input >> DenseLayer >> AffineLayer >> softmax loss >> output\n",
    "\n",
    "```\n",
    "    class TwoLayerNet:\n",
    "        __init__: \n",
    "            - layers: a dense layer and an affine layer\n",
    "        forward: forward pass\n",
    "        loss: cross entropy loss and gradients\n",
    "        step: a single step update of all weights and bias by SGD.\n",
    "        predict: output result (classification accuracy) based on input data\n",
    "        save_model: return the parameters of the network\n",
    "        update_model: update model weights (by calling layer.update_layer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use two different optimization algorithms - **SGD** and **SGD with momentum**. A comparison between them is as follows:\n",
    "\n",
    "* Stochastic gradient descent - SGD\n",
    "\n",
    "$$w \\gets w - \\alpha \\nabla_w L$$\n",
    "\n",
    "* SGD with momentum\n",
    "\n",
    "$$v \\gets \\beta v + \\alpha \\nabla_w L,\\quad w \\gets w - v$$\n",
    "\n",
    "where $\\alpha$ is the learning rate (step size) and $\\beta$ is the momentum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBFtQx5Utrpu"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete class `TwoLayerNet` in `./utils/classifiers/twolayernet.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below. The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1QlpmRrEtrpv",
    "outputId": "2dfb94b6-1d2d-4bf6-efce-0d54c7725cc8",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m## Backprogation -- Finish loss function and gradients calculation in TwoLayerNet\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(y_score, y_dev)\n\u001b[0;32m---> 14\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_dev)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m## Check loss by tensorflow\u001b[39;00m\n\u001b[1;32m     17\u001b[0m x_tf \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(X_dev, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/Downloads/e4040-2024fall-assign1-Lindsey-cyber-main/utils/classifiers/twolayernet.py:185\u001b[0m, in \u001b[0;36mTwoLayerNet.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    176\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# TODO: Generate predictions                                               #\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m#                         START OF YOUR CODE                               #\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m#                          END OF YOUR CODE                                #\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.classifiers.twolayernet import TwoLayerNet\n",
    "\n",
    "## Define a model\n",
    "model = TwoLayerNet(input_dim=X_train.shape[1], hidden_dim=100, num_classes=20, reg=1e-4)\n",
    "W1, b1 = model.layer1.params['W'], model.layer1.params['b']\n",
    "W2, b2 = model.layer2.params['W'], model.layer2.params['b']\n",
    "## Feedforward\n",
    "y_score = model.forward(X_dev)\n",
    "## Backprogation -- Finish loss function and gradients calculation in TwoLayerNet\n",
    "loss = model.loss(y_score, y_dev)\n",
    "pred = model.predict(X_dev)\n",
    "\n",
    "## Check loss by tensorflow\n",
    "x_tf = tf.Variable(X_dev, dtype=tf.float32)\n",
    "y_tf = tf.Variable(y_dev, dtype=tf.uint8)\n",
    "\n",
    "W1_tf = tf.Variable(W1.astype('float32'))\n",
    "b1_tf = tf.Variable(b1.astype('float32'))\n",
    "W2_tf = tf.Variable(W2.astype('float32'))\n",
    "b2_tf = tf.Variable(b2.astype('float32'))\n",
    "h1_tf = tf.nn.relu(tf.matmul(x_tf, W1_tf))\n",
    "h2_tf = tf.matmul(h1_tf, W2_tf) + b2_tf\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=h2_tf, labels=tf.one_hot(y_tf, 20))\n",
    "L2_loss = tf.nn.l2_loss(W1_tf) + tf.nn.l2_loss(W2_tf)\n",
    "loss_tf = tf.reduce_mean(cross_entropy) + 1e-4 * L2_loss\n",
    "pred_tf = tf.argmax(h2_tf, axis=-1)\n",
    "\n",
    "loss_check = loss_tf.numpy()\n",
    "pred_check = pred_tf.numpy()\n",
    "\n",
    "## Print validation result\n",
    "print(\"Is loss correct? {}\".format(np.allclose(loss, loss_check)))\n",
    "print(\"Is prediction correct? {}\".format(np.allclose(pred, pred_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WjSapZXtrpy"
   },
   "source": [
    "### Train a two-layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HnEd1Z9Wtrpz"
   },
   "source": [
    "#### Import functions for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "et5ZTXBktrpz"
   },
   "outputs": [],
   "source": [
    "from utils.train_funcs import train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2Nbd9iotrp2"
   },
   "source": [
    "#### Start training\n",
    "We have provide you the `train` function in **./utils/train_func.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "eOIqjSUCtrp2",
    "outputId": "8cc8db83-6caa-4357-e086-87801f6fabf1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 98\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'layer_funcs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5e-4\u001b[39m\n\u001b[1;32m     16\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m train_acc_hist, val_acc_hist \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m     18\u001b[0m     model, X_train, y_train, X_val, y_val, num_epoch\u001b[38;5;241m=\u001b[39mnum_epoch, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[1;32m     19\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mlr, verbose\u001b[38;5;241m=\u001b[39mverbose\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m test(model, X_test, y_test)\n",
      "File \u001b[0;32m~/Downloads/e4040-2024fall-assign1-Lindsey-cyber-main/utils/train_funcs.py:47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, X_train, y_train, X_valid, y_valid, num_epoch, batch_size, learning_rate, learning_decay, optim, momentum, verbose)\u001b[0m\n\u001b[1;32m     45\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_train[sample_idxs]\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(X_batch)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(scores, y_batch)\n",
      "File \u001b[0;32m~/Downloads/e4040-2024fall-assign1-Lindsey-cyber-main/utils/classifiers/twolayernet.py:55\u001b[0m, in \u001b[0;36mTwoLayerNet.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03mFeed forward\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mReturns a numpy array of (N, K) containing prediction scores\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#                        START OF YOUR CODE                                #\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# NOTE: Use self.layer#.feedforward function you wrote in layer_funcs.py   #\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlayer_funcs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m affine_forward, relu_forward, tanh_forward\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#                          END OF YOUR CODE                                #\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'layer_funcs'"
     ]
    }
   ],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE     #\n",
    "# DO NOT CHANGE IT.                        #\n",
    "\n",
    "from utils.classifiers.twolayernet import TwoLayerNet\n",
    "\n",
    "## TODO: Use previous layers to create a two layer neural network\n",
    "## input->(affine->activation)->(affine->softmax)->output\n",
    "## The recommended activation function is ReLU. And you can \n",
    "## also make a comparison with other activation function to see\n",
    "## any difference.\n",
    "model = TwoLayerNet(input_dim=X_train.shape[1], hidden_dim=400, num_classes=20, reg=1e-4, weight_scale=1e-3)\n",
    "\n",
    "num_epoch = 10\n",
    "batch_size = 500\n",
    "lr = 5e-4\n",
    "verbose = True\n",
    "train_acc_hist, val_acc_hist = train(\n",
    "    model, X_train, y_train, X_val, y_val, num_epoch=num_epoch, batch_size=batch_size, \n",
    "    learning_rate=lr, verbose=verbose\n",
    ")\n",
    "test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ID-4_nLqtrp5"
   },
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Plot training and validation accuracy history of each epoch. Remember to add a legend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Plot the accuracy history\n",
    "\n",
    "############################################################################\n",
    "#                         START OF YOUR CODE                               #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                          END OF YOUR CODE                                #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42xHlu5utrp9"
   },
   "source": [
    "#### Visulize the weight variable in the first layer.\n",
    "\n",
    "Visualization of the intermediate weights can help you get an intuitive understanding of how the network works, especially in  Convolutional Neural Networks (CNNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gFr5Kvrmtrp9"
   },
   "outputs": [],
   "source": [
    "from utils.display_funcs import visualize_pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "OrusGTkltrqA",
    "outputId": "6a87d009-7825-4795-b23d-95ba44a5cf8d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = model.layer1.params['W']\n",
    "pics = weights.reshape(1, X_train_raw.shape[1], X_train_raw.shape[2], -1).transpose(3, 1, 2, 0)\n",
    "## Visualization\n",
    "visualize_pics(pics, cmap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2s4-QzTHtrqD"
   },
   "source": [
    "### Get test accuracy greater than 80%\n",
    "\n",
    "For this part, you need to train a better two-layer net. The requirement is to get test accuracy better than 80%. If your accuracy is lower, for each 1% lower than 80%, you will lose 1 point (There are totally 10 points for this part).\n",
    "\n",
    "Here are some recommended methods for improving the performance. Feel free to try any other method as you see fit.\n",
    "\n",
    "1. Hyperparameter tuning: reg, hidden_dim, lr, learning_decay, num_epoch, batch_size, weight_scale.\n",
    "2. Adjust training strategy: Randomly select a batch of samples rather than selecting them orderly. \n",
    "3. Try new optimization methods: Now we are using SGD, you can try SGD with momentum, adam, etc.\n",
    "4. Early-stopping.\n",
    "5. Good (better) initial values for weights in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XK0pOcTLtrqE"
   },
   "source": [
    "<font color=\"red\"><strong>TODO:</strong></font> See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfTHBopktrqE"
   },
   "outputs": [],
   "source": [
    "from utils.classifiers.twolayernet import TwoLayerNet\n",
    "# TODO: Use previous layers to create a two layer neural network.\n",
    "# Try several solutions and report the best performing one.\n",
    "# input->(affine->activation)->(affine->softmax)->output\n",
    "# The recommended activation function is ReLU. You can \n",
    "# make a comparison with other activation functions to see\n",
    "# the differences.\n",
    "#\n",
    "# You will need to execute code similar to the code below, using your parameter specs:\n",
    "#    model = TwoLayerNet(input_dim=TBD, hidden_dim=TBD, num_classes=TBD, reg=TBD, weight_scale=TBD)\n",
    "#    num_epoch = TBD\n",
    "#    batch_size = TBD\n",
    "#    lr = TBD\n",
    "#    verbose = TBD\n",
    "#    train_acc_hist, val_acc_hist = train(TBD)\n",
    "#    test(TBD, TBD, TBD)\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                         START OF YOUR CODE                               #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                          END OF YOUR CODE                                #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qI9Zc9cItrqJ"
   },
   "source": [
    "#### <font color=\"red\"><strong>TODO</strong></font>: Show your best result, including accuracy and weights of the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Plot training and validation accuracy\n",
    "\n",
    "############################################################################\n",
    "#                         START OF YOUR CODE                               #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                          END OF YOUR CODE                                #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Visualize weights\n",
    "\n",
    "############################################################################\n",
    "#                         START OF YOUR CODE                               #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                          END OF YOUR CODE                                #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7SAy1J1trqP"
   },
   "source": [
    "### Save Your Best Model in a Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9iMeOm1trqQ"
   },
   "outputs": [],
   "source": [
    "## Create \"save_model\" folder if it does not exist\n",
    "save_dir = \"./save_models/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "## Save your model\n",
    "save_params = model.save_model()\n",
    "with open(\"./save_models/best_model.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(save_params, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XzKlA_1trqT"
   },
   "outputs": [],
   "source": [
    "## Load your model\n",
    "with open(\"./save_models/best_model.pkl\", \"rb\") as input_file:\n",
    "   loaded_model = pickle.load(input_file)\n",
    "\n",
    "model = TwoLayerNet(input_dim=X_train.shape[1], hidden_dim=300, num_classes=20)\n",
    "model.update_model(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6DUwt8OtrqV"
   },
   "source": [
    "## Part 3: Multilayer Network (10%)\n",
    "\n",
    "Complete the class `MLP` in `./utils/classifiers/mlp.py`. It should allow arbitrary settings for the number of hidden layers as well as the number of hidden neurons in each layer. `MLP` has a similar structure as a `TwoLayerNet` network.\n",
    "\n",
    "```\n",
    "class MLP:\n",
    "    functions: __init__, loss, step, predict, check_accuracy\n",
    "    variables: layers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Complete the class `MLP` in `./utils/classifiers/mlp.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>NOTE</strong></font>: Please do not change the code in the cell below, The cell below will run correctly if your code is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "1C7yH-HBtrqW",
    "outputId": "718a8627-54d1-456f-afb4-3239c3564191",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THE FOLLOWING IS THE VERIFICATION CODE\n",
    "# DO NOT CHANGE IT.\n",
    "\n",
    "from utils.classifiers.mlp import MLP\n",
    "\n",
    "## Use a sequence of layers to create a multiple layer neural network\n",
    "## input->(affine->activation)-> ... ->(affine->activation)->(affine->softmax)->output\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dims=[200, 100], num_classes=20, reg=0.5, weight_scale=1e-3)\n",
    "\n",
    "num_epoch = 10\n",
    "batch_size = 128\n",
    "lr = 5e-3\n",
    "verbose = False\n",
    "train_acc_hist, val_acc_hist = train(\n",
    "    model, X_train, y_train, X_val, y_val, \n",
    "    num_epoch=num_epoch, batch_size=batch_size, learning_rate=lr, \n",
    "    optim='SGD', momentum=0.9, verbose=verbose\n",
    ")\n",
    "test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><strong>TODO</strong></font>: Plot training and validation accuracy history of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: plot training and validation accuracy\n",
    "\n",
    "############################################################################\n",
    "#                         START OF YOUR CODE                               #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                          END OF YOUR CODE                                #\n",
    "############################################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task2-mlp_eager.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "36142657f443a869bd2c1b509e6f1df9b014ad48aa206cdd00d27f8f22cb37ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
